The past five years have seen a huge increase in the capabilities of deep neural networks.
Maintaining this rate of progress however, faces some steep challenges, and awaits fundamental insights.
As our models become more complex and venture into areas such as unsupervised learning or reinforcement learning, designing improvements becomes more laborious and success can be brittle and hard to transfer to new settings.

This workshop seeks to highlight recent works that use theory as well as systematic experiments to isolate the fundamental questions that need to be addressed in deep learning.
These works have helped flesh out core questions on topics such as generalization, adversarial robustness, large batch training, generative adversarial nets, and optimization, and point towards elements of the theory of deep learning that is expected to emerge in the future.

The workshop aims to enhance this confluence of theory and practice, highlighting influential work with these methods, future open directions, and core fundamental problems. There will be an emphasis on discussion to identify future research directions that are promising and tractable.

# Confirmed Speakers
[Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/) University of Montreal
[Ian Goodfellow](http://www.iangoodfellow.com/) Google Brain
[Sham Kakade](https://homes.cs.washington.edu/~sham/)
Percy Liang
Nati Srebro

# Call for Papers


## Submission Instructions

No special style is required. Authors may use the NIPS style file, but are also free to use other styles as long as they use standard font size (11 pt) and margins (1 in).



# Organizers
- [Sanjeev Arora](https://www.cs.princeton.edu/~arora/)
- Maithra Raghu
- Ruslan Salakhutdinov
- Ludwig Schmidt
- Oriol Vinyals
